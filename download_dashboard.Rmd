---
title: "Subgraph Query Volume Data"
output:
  flexdashboard::flex_dashboard:
    orientation: columns
    vertical_layout: fill
    theme: spacelab
runtime: shiny
---

```{r setup, include=FALSE, cache=TRUE}
library(flexdashboard)
library(shiny)
library(dplyr)
library(ggplot2)
library(DT)
library(lubridate)
library(plotly)  # Add this line to import plotly
library(scales)  # Add this line to import scales
library(pins)
library(httr)
library(jsonlite)
# library(googleCloudStorageR)
# gcs_setup() # if needed for initial setup

# connect to pins board
# board = board_gcs('indexer-data-qos', prefix = NULL, versioned = FALSE, cache = NULL)

# result = board %>% pin_read("qos_subgraph_subgraph_level_full_historical")


# Function to make GraphQL query
make_query <- function(query, variables = list()) {
  api_key <- Sys.getenv("THEGRAPH_API_KEY")
  if (api_key == "") {
    stop("THEGRAPH_API_KEY environment variable is not set")
  }
  url <- paste0("https://gateway.thegraph.com/api/", api_key, "/subgraphs/id/Dtr9rETvwokot4BSXaD5tECanXfqfJKcvHuaaEgPDD2D")
  
  response <- POST(
    url,
    body = list(query = query, variables = variables),
    encode = "json"
  )
  
  if (status_code(response) != 200) {
    stop("API request failed with status code: ", status_code(response))
  }
  
  content <- content(response, "text")
  parsed <- fromJSON(content)
  
  if (!is.null(parsed$errors)) {
    stop("GraphQL query error: ", toJSON(parsed$errors))
  }
  
  return(parsed$data$queryDataPoints)
}

# Updated function to fetch data
fetch_data <- function(batch_size = 1000, max_rows = 100000) {
  all_data <- data.frame()
  skip <- 0
  last_end_epoch <- 9999999999.0  # Use a large number as a float
  
  while (nrow(all_data) < max_rows) {
    query <- sprintf('
    {
      queryDataPoints(
        first: %d
        skip: %d
        orderBy: end_epoch
        orderDirection: desc
        where: {end_epoch_lt: %.1f}
      ) {
        end_epoch
        chain_id
        gateway_query_success_rate
        query_count
        total_query_fees
        user_attributed_error_rate
        avg_gateway_latency_ms
        avg_query_fee
        subgraph_deployment_ipfs_hash
      }
    }
    ', batch_size, skip, last_end_epoch)
    
    tryCatch({
      batch_data <- make_query(query)
      
      if (is.null(batch_data) || nrow(batch_data) == 0) {
        cat("No more data to fetch.\n")
        break
      }
      
      all_data <- bind_rows(all_data, batch_data)
      last_end_epoch <- min(as.numeric(batch_data$end_epoch))
      
      cat(sprintf("Fetched %d rows. Total: %d\n", nrow(batch_data), nrow(all_data)))
      
      if (nrow(batch_data) < batch_size) {
        cat("Reached the end of available data.\n")
        break
      }
      
      # Reset skip every 5,000 rows
      if (nrow(all_data) %% 5000 < batch_size) {
        skip <- 0
        cat("Reset skip to 0.\n")
      } else {
        skip <- skip + nrow(batch_data)
      }
      
    }, error = function(e) {
      cat("Error occurred:", conditionMessage(e), "\n")
      cat("Retrying in 5 seconds...\n")
      Sys.sleep(5)
    })
  }
  
  return(all_data)
}

# Main execution
cat("Starting data check...\n")

csv_file <- "subgraph_data.csv"
current_time <- Sys.time()

if (file.exists(csv_file)) {
  file_info <- file.info(csv_file)
  time_diff <- as.numeric(difftime(current_time, file_info$mtime, units = "mins"))
  
  if (time_diff <= 30) {
    # cat("Data has been pulled within the last 30 minutes. Skipping fetch.\n")
  } else {
    # cat("Data is older than 30 minutes. Fetching new data...\n")
    result <- fetch_data(batch_size = 1000, max_rows = 100000)
    # cat(sprintf("\nTotal rows fetched: %d\n", nrow(result)))
    write.csv(result, csv_file, row.names = FALSE)
    # cat("Data saved to subgraph_data.csv\n")
  }
} else {
  # cat("CSV file does not exist. Fetching new data...\n")
  result <- fetch_data(batch_size = 1000, max_rows = 100000)
  # cat(sprintf("\nTotal rows fetched: %d\n", nrow(result)))
  write.csv(result, csv_file, row.names = FALSE)
  # cat("Data saved to subgraph_data.csv\n")
}

# Read the CSV file
result <- read.csv(csv_file)

# Data preprocessing
result$date <- as.POSIXct(as.numeric(result$end_epoch), origin="1970-01-01", tz="UTC")
result$query_count <- as.numeric(result$query_count)

# Calculate date range
max_date <- max(result$date)
min_date <- min(result$date)

# Get unique chain_ids
chain_ids = sort(unique(result$chain_id))
```


Subgraph Overview
===================================

Column {.sidebar}
-----------------------------------------------------------------------

```{r}
selectInput("chain_filter", "Select Chain ID:",
            choices = c("All", chain_ids),
            selected = "All",
            multiple = TRUE)  # Change this line

# Function to filter data based on chain_id
filter_by_chain = function(data) {
  if ("All" %in% input$chain_filter || length(input$chain_filter) == 0) {
    data
  } else {
    data %>% filter(chain_id %in% input$chain_filter)
  }
}
```

Column {.tabset}
-----------------------------------------------------------------------

### Top Subgraphs by Query Volume

```{r}
renderDataTable({
  filtered_result = filter_by_chain(result)
  
  top_subgraphs = filtered_result %>%
    group_by(subgraph_deployment_ipfs_hash) %>%
    summarize(total_queries = sum(query_count)) %>%
    arrange(desc(total_queries)) %>%
    head(100)
  
  datatable(top_subgraphs,
            escape = FALSE,
            options = list(pageLength = 10, scrollX = TRUE))
})
```

### Top Subgraphs by Query Volume Chart

```{r}
renderPlotly({
  filtered_result = filter_by_chain(result)
  
  top_10_subgraphs = filtered_result %>%
    group_by(subgraph_deployment_ipfs_hash) %>%
    summarize(total_queries = sum(query_count)) %>%
    top_n(10, total_queries) %>%
    pull(subgraph_deployment_ipfs_hash)
  
  filtered_data = filtered_result %>%
    filter(subgraph_deployment_ipfs_hash %in% top_10_subgraphs)
  
  p = ggplot(filtered_data, aes(x = date, y = query_count, color = subgraph_deployment_ipfs_hash)) +
    geom_line() +
    labs(title = "Top 10 Subgraphs by Query Volume",
         x = "Date",
         y = "Query Count",
         color = "Subgraph ID") +
    theme_minimal() +
    theme(legend.position = "bottom") +
    scale_y_continuous(labels = scales::comma)
  
  ggplotly(p)
})
```

### Top Volume Increases

```{r}
volume_increase = reactive({
  filtered_result = filter_by_chain(result)
  
  filtered_result %>%
    group_by(subgraph_deployment_ipfs_hash) %>%
    arrange(date) %>%
    summarize(
      start_volume = first(query_count),
      end_volume = last(query_count),
      start_date = first(date),
      end_date = last(date),
      volume_increase = end_volume - start_volume,
      hours_difference = as.numeric(difftime(end_date, start_date, units = "hours"))
    ) %>%
    filter(hours_difference > 0) %>%  # Ensure at least some time difference
    mutate(hourly_increase = volume_increase / hours_difference) %>%
    arrange(desc(hourly_increase)) %>%
    head(100)
})

renderDataTable({
  datatable(volume_increase(),
            escape = FALSE,
            options = list(pageLength = 10, scrollX = TRUE))
})
```

### Top Volume Increases Chart

```{r}
renderPlotly({
  filtered_result = filter_by_chain(result)
  
  top_increase_subgraphs = volume_increase() %>%
    head(10) %>%  # Take only top 10
    pull(subgraph_deployment_ipfs_hash)
  
  filtered_data = filtered_result %>%
    filter(subgraph_deployment_ipfs_hash %in% top_increase_subgraphs) %>%
    group_by(subgraph_deployment_ipfs_hash) %>%
    arrange(date) %>%
    slice(c(1, n()))  # Keep only first and last data points
  
  p = ggplot(filtered_data, aes(x = date, y = query_count, color = subgraph_deployment_ipfs_hash, group = subgraph_deployment_ipfs_hash)) +
    geom_line() +
    geom_point(size = 3) +
    labs(title = "Top 10 Query Volume Increases: First vs Last Available Hour",
         x = "Date",
         y = "Query Count",
         color = "Subgraph ID") +
    theme_minimal() +
    theme(legend.position = "bottom") +
    scale_y_continuous(labels = scales::comma)
  
  ggplotly(p)
})
```

Raw Data
===================================

Column {.sidebar}
-----------------------------------------------------------------------

```{r}
selectInput("chain_filter2", "Select Chain ID:",
            choices = c("All", chain_ids),
            selected = "All",
            multiple = TRUE)  # Change this line
```

Column
-----------------------------------------------------------------------

### Raw Data

```{r}
renderDataTable({
  if ("All" %in% input$chain_filter2 || length(input$chain_filter2) == 0) {
    table_data = result
  } else {
    table_data = result %>% filter(chain_id %in% input$chain_filter2)
  }
  
  datatable(table_data,
            escape = FALSE,
            extensions = "Buttons",
            options = list(
              scrollX = TRUE,
              scrollY = "500px",
              paging = TRUE,
              searching = TRUE,
              ordering = TRUE,
              dom = 'Bfrtip',
              buttons = c('copy', 'csv', 'excel', 'pdf'),
              pageLength = 50
            ))
})
```

All Subgraphs Visualization
===================================

Column
-----------------------------------------------------------------------

### Total Query Count Across All Subgraphs

```{r}
renderPlotly({
  filtered_result = filter_by_chain(result)
  
  # Aggregate data: sum query_count for each end_epoch
  aggregated_data = filtered_result %>%
    group_by(date) %>%
    summarize(total_query_count = sum(query_count, na.rm = TRUE))
  
  p = ggplot(aggregated_data, aes(x = date, y = total_query_count)) +
    geom_line() +
    geom_point(size = 2) +
    labs(title = "Total Query Count Across All Subgraphs",
         x = "Date",
         y = "Total Query Count") +
    theme_minimal() +
    scale_y_continuous(labels = scales::comma)
  
  ggplotly(p)
})
```


By Chain
===================================

Column {.sidebar}
-----------------------------------------------------------------------

```{r}
selectInput("chain_filter3", "Select Chain ID:",
            choices = c("All", chain_ids),
            selected = "All",
            multiple = TRUE)
```


Column {.tabset}
-----------------------------------------------------------------------

### Query Volume by Chain

```{r}
renderPlotly({
  if ("All" %in% input$chain_filter3 || length(input$chain_filter3) == 0) {
    filtered_data = result
  } else {
    filtered_data = result %>% filter(chain_id %in% input$chain_filter3)
  }
  
  chain_summary = filtered_data %>%
    group_by(chain_id) %>%
    summarize(sum_queries = sum(query_count)) %>%
    arrange(desc(sum_queries))
  
  p = ggplot(chain_summary, aes(x = reorder(chain_id, -sum_queries), y = sum_queries)) +
    geom_bar(stat = "identity", fill = "steelblue") +
    labs(title = "Total Query Volume by Chain",
         x = "Chain ID",
         y = "Total Query Count") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    scale_y_continuous(labels = scales::comma)
  
  ggplotly(p)
})
```

### Chain Query Volume Over Time


```{r}
renderPlotly({
  if ("All" %in% input$chain_filter3 || length(input$chain_filter3) == 0) {
    filtered_data = result
  } else {
    filtered_data = result %>% filter(chain_id %in% input$chain_filter3)
  }
  
  time_series_data = filtered_data %>%
    group_by(chain_id, date) %>%
    summarize(total_queries = sum(query_count))
  
  p = ggplot(time_series_data, aes(x = date, y = total_queries, color = chain_id)) +
    geom_line() +
    labs(title = "Query Volume Over Time by Chain",
         x = "Date",
         y = "Query Count",
         color = "Chain ID") +
    theme_minimal() +
    theme(legend.position = "bottom") +
    scale_y_continuous(labels = scales::comma)
  
  ggplotly(p)
})
```


### Chain Summary Table

```{r}
renderDataTable({
  if ("All" %in% input$chain_filter3 || length(input$chain_filter3) == 0) {
    filtered_data = result
  } else {
    filtered_data = result %>% filter(chain_id %in% input$chain_filter3)
  }
  
  chain_summary = filtered_data %>%
    group_by(chain_id) %>%
    summarize(
      total_queries = sum(query_count),
      avg_query_fee = mean(avg_query_fee, na.rm = TRUE),
      avg_gateway_latency = mean(avg_gateway_latency_ms, na.rm = TRUE),
      avg_success_rate = mean(gateway_query_success_rate, na.rm = TRUE)
    ) %>%
    arrange(desc(total_queries))
  
  datatable(chain_summary,
            options = list(pageLength = 10, scrollX = TRUE),
            rownames = FALSE) %>%
    formatRound(columns = c("total_queries", "avg_query_fee", "avg_gateway_latency", "avg_success_rate"), digits = 2)
})
```
